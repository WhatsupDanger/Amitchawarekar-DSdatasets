##->Practical 1 Data Types with R Language<-

```
v <- TRUE 
print(class(v))
v <- 23.5
print(class(v)) 
v <- 2L
print(class(v))
v <- 2+5i
print(class(v)) 
v <- "TRUE"
print(class(v))
v <- charToRaw("Hello")
print(class(v))
```
##->Practical 1 Data Types with R Language (Vector Form)<-
```
apple <- c('red','green',"yellow")
print(apple)
# Get the class of the vector.
print(class(apple))
# Creating a sequence from 5 to 13.
v <- 5:13
print(v)
# Creating a sequence from 6.6 to 12.6.
v <- 6.6:12.6
print(v)
# If the final element specified does not belong to the sequence then it is discarded. 
v <- 3.8:11.4
print(v)
#Lists
# Create a list. 
list1 <- list(c(2,5,3),21.3,sin)
# Print the list.
print(list1)
# values.
list_data <- list("Red", "Green", c(21,32,11), TRUE, 51.23, 119.1)
print(list_data)
#Matrix 
# Create a matrix.
M = matrix( c('a','a','b','c','b','a'), nrow = 2, ncol = 3, byrow = TRUE)
print(M)
#Arrays 
# Create an array.
a <- array(c('green','yellow'),dim = c(3,3,2))
print(a)
#Factors
# Create a vector. 
apple_colors <- c('green','green','yellow','red','red','red','green')
# Create a factor object.
factor_apple <- factor(apple_colors)
# Print the factor.
print(factor_apple)
print(nlevels(factor_apple))
#Data Frames
# Create the data frame. 
BMI <- data.frame(
  gender = c("Male", "Male","Female"), 
  height = c(152, 171.5, 165),
  weight = c(81,93, 78),
  Age = c(42,38,26) 
)
print(BMI)
```
##->Practical 2 Data Cleaning and EDA (reading, dropping null values,summary,mean,operation on rows&cols, condition on subset, sorting,joins)<-

```
install.packages(c("cluster", "magrittr", "mgcv", "readxl", "tzdb", "vctrs", "tidyverse", "repr", "corrplot", "ggplot2", "caret", "pROC", "kernlab", "randomForest", "forecast", "dplyr"))
library(tidyverse)
data <- read.csv("heart.csv")
head(data)
tail(data)
glimpse(data)
str(data)
min(data)
max(data)
ncol(data)
nrow(data)
colnames(data)
summary(data)
```

##->Practical 3 Data Visualization with ggplot,geom functions,aesthatic function and facet  *run prac2 to get prac 3 output<-

```
#Data Transformation
data2 <- data %>%
  mutate(sex = if_else(sex == 1, "MALE", "FEMALE"),
         fbs = if_else(fbs == 1, ">120", "<=120"),
         exang = if_else(exang == 1, "YES" ,"NO"),
         cp = if_else(cp == 1, "ATYPICAL ANGINA",
                      if_else(cp == 2, "NON-ANGINAL PAIN", "ASYMPTOMATIC")),
         restecg = if_else(restecg == 0, "NORMAL",
                           if_else(restecg == 1, "ABNORMALITY", "PROBABLE OR DEFINITE")),
         slope = as.factor(slope),
         ca = as.factor(ca),
         thal = as.factor(thal),
         target = if_else(target == 1, "YES", "NO")
  ) %>% 
  mutate_if(is.character, as.factor) %>% 
  dplyr::select(target, sex, fbs, exang, cp, restecg, slope, ca, thal, everything())

#Data Visualization

library(repr)
#change the size of the plot

options(repr.plot.width =6, repr.plot.height =3)
# Bar Plot for target (heart disease)

ggplot(data2, aes(x=data2$target, fill=data2$target))+
  geom_bar()+
  xlab("Heart Disease")+
  ylab("count")+
  ggtitle("Presence & Absence of Heart Disease")+
  scale_fill_discrete(name= 'Heart Disease', labels =c("Absence", "Presence"))

prop.table(table(data2$target))

# count the frequency of the values of age

data2 %>%
  group_by(ï..age) %>%
  count() %>%
  filter(n>10) %>%
  ggplot()+
  geom_col(aes(ï..age, n), fill = 'green')+
  ggtitle("Age Analysis")+
  xlab("Age")+
  ylab("Agecount")

# comapre blood pressure across the chest pain

data2 %>%
  ggplot(aes(x=sex, y=trestbps))+
  geom_boxplot(fill ='purple')+
  xlab('sex')+
  ylab('BP')+
  facet_grid(~cp)

data2 %>%
  ggplot(aes(x=sex, y=chol))+
  geom_boxplot(fill ='orange')+
  xlab('sex')+
  ylab('Chol')+
  facet_grid(~cp)

library(corrplot)
library(ggplot2)

cor_heart <- cor(data2[, 10:14])
cor_heart

corrplot(cor_heart, method ='square', type='upper')

#Check for the null values
s = sum(is.na(data2))
s
is.null(data2) #another way
```

##->Practical 5 Data Modelling		*Run prac 2-3 to get output<-

```
library(caret)
set.seed(10)
colnames(data2)
inTrainRows <- createDataPartition(data2$target,p=0.7,list=FALSE)
trainData <- data2[inTrainRows,]
testData <-  data2[-inTrainRows,]
nrow(trainData)/(nrow(testData)+nrow(trainData)) #check the percentage
AUC = list()
Accuracy = list()
set.seed(10)
logRegModel <- train(target ~ ., data=trainData, method = 'glm', family = 'binomial')
logRegPrediction <- predict(logRegModel, testData)
logRegPredictionprob <- predict(logRegModel, testData, type='prob')[2]
logRegConfMat <- confusionMatrix(logRegPrediction, testData[,"target"])
#ROC Curve
library(pROC)
AUC$logReg <- roc(as.numeric(testData$target),as.numeric(as.matrix((logRegPredictionprob))))$auc
Accuracy$logReg <- logRegConfMat$overall['Accuracy']  #found names with str(logRegConfMat)
set.seed(10)
logRegModel <- train(target ~ ., data=trainData, method = 'glm', family = 'binomial')
logRegPrediction <- predict(logRegModel, testData)
logRegPredictionprob <- predict(logRegModel, testData, type='prob')[2]
logRegConfMat <- confusionMatrix(logRegPrediction, testData[,"target"])
#ROC Curve
library(pROC)
AUC$logReg <- roc(as.numeric(testData$target),as.numeric(as.matrix((logRegPredictionprob))))$auc
Accuracy$logReg <- logRegConfMat$overall['Accuracy']  #found names with str(logRegConfMat)

#Support Vector Machine
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 10,
                           ## Estimate class probabilities
                           classProbs = TRUE,
                           ## Evaluate performance using 
                           ## the following function
                           summaryFunction = twoClassSummary)
library(kernlab)
set.seed(10)
svmModel <- train(target ~ ., data = trainData,
                  method = "svmRadial",
                  trControl = fitControl,
                  preProcess = c("center", "scale"),
                  tuneLength = 8,
                  metric = "ROC")
svmPrediction <- predict(svmModel, testData)
svmPredictionprob <- predict(svmModel, testData, type='prob')[2]
svmConfMat <- confusionMatrix(svmPrediction, testData[,"target"])
#ROC Curve
AUC$svm <- roc(as.numeric(testData$target),as.numeric(as.matrix((svmPredictionprob))))$auc
Accuracy$svm <- svmConfMat$overall['Accuracy']


#Random Forest
library(randomForest)
set.seed(10)
RFModel <- randomForest(target ~ .,
                        data=trainData, 
                        importance=TRUE, 
                        ntree=200)
#varImpPlot(RFModel)
RFPrediction <- predict(RFModel, testData)
RFPredictionprob = predict(RFModel,testData,type="prob")[, 2]

RFConfMat <- confusionMatrix(RFPrediction, testData[,"target"])

AUC$RF <- roc(as.numeric(testData$target),as.numeric(as.matrix((RFPredictionprob))))$auc
Accuracy$RF <- RFConfMat$overall['Accuracy']

#Comparison of AUC and Accuracy between models
row.names <- names(Accuracy)
col.names <- c("AUC", "Accuracy")
cbind(as.data.frame(matrix(c(AUC,Accuracy),nrow = 3, ncol = 2,
                           dimnames = list(row.names, col.names))))

summary(logRegModel)$coeff

#Confusion Matrix
logRegConfMat
RFConfMat
svmConfMat
#A comparison of the area under the ROC and the accuracy of the model predictions shows that logistic regression performs best (accuracy of 0.87).Tree-based methods shows low accuracy.
```

##->Practical 6 (Linear,Multiple,Logistic,Poisson,dnorm,pnorm,qnorm,rnorm,dbinom,qbinom,rbinom)<-

```
#Linear Regression
x <- c(151, 174, 138, 186, 128, 136, 179, 163, 152, 131)
y <- c(63, 81, 56, 91, 47, 57, 76, 72, 62, 48)
# Apply the lm() function.
relation <- lm(y~x)
print(relation)
#Get the Summary of the Relationship
print(summary(relation))
# Find weight of a person with height 170.
a <- data.frame(x = 170)
result <-  predict(relation,a)
print(result)
# Give the chart file a name.
png(file = "linearregression.png")
# Plot the chart.
plot(y,x,col = "blue",main = "Height & Weight Regression",
     abline(lm(x~y)),cex = 1.3,pch = 16,xlab = "Weight in Kg",ylab = "Height in cm")
# Save the file.
dev.off()

#Multiple Regression
input <- mtcars[,c("mpg","disp","hp","wt")]
# Create the relationship model.
model <- lm(mpg~disp+hp+wt, data = input)
# Show the model.
print(model)
# Get the Intercept and coefficients as vector elements.
cat("# # # # The Coefficient Values # # # ","\n")
a <- coef(model)[1]
print(a)
Xdisp <- coef(model)[2]
Xhp <- coef(model)[3]
Xwt <- coef(model)[4]
print(Xdisp)
print(Xhp)
print(Xwt)
#For a car with disp = 221, hp = 102 and wt = 2.91 the predicted mileage is −
Y = 37.15+(-0.000937)*221+(-0.0311)*102+(-3.8008)*2.91

#Logistic Regression
input <- mtcars[,c("am","cyl","hp","wt")]
print(head(input))
#Create Regression Model
am.data = glm(formula = am ~ cyl + hp + wt, data = input, family = binomial)
print(summary(am.data))

#Poisson Regression
input <- warpbreaks
print(head(input))
#Create Regression Model
output <-glm(formula = breaks ~ wool+tension, data = warpbreaks,
             family = poisson)
print(summary(output))

#dnorm
# Create a sequence of numbers between -10 and 10 incrementing by 0.1.
x <- seq(-10, 10, by = .1)
# Choose the mean as 2.5 and standard deviation as 0.5.
y <- dnorm(x, mean = 2.5, sd = 0.5)
# Give the chart file a name.
png(file = "dnorm.png")
plot(x,y)
# Save the file.
dev.off()

#pnorm
# Create a sequence of numbers between -10 and 10 incrementing by 0.2.
x <- seq(-10,10,by = .2)
# Choose the mean as 2.5 and standard deviation as 2. 
y <- pnorm(x, mean = 2.5, sd = 2)
# Give the chart file a name.
png(file = "pnorm.png")
# Plot the graph.
plot(x,y)
# Save the file.
dev.off()

#qnorm
# Create a sequence of probability values incrementing by 0.02.
x <- seq(0, 1, by = 0.02)
# Choose the mean as 2 and standard deviation as 3.
y <- qnorm(x, mean = 2, sd = 1)
# Give the chart file a name.
png(file = "qnorm.png")
# Plot the graph.
plot(x,y)
# Save the file.
dev.off()

#rnorm
# Create a sample of 50 numbers which are normally distributed.
y <- rnorm(50)
# Give the chart file a name.
png(file = "rnorm.png")
# Plot the histogram for this sample.
hist(y, main = "Normal DIstribution")
# Save the file.
dev.off()

#dbinom
# Create a sample of 50 numbers which are incremented by 1.
x <- seq(0,50,by = 1)
# Create the binomial distribution.
y <- dbinom(x,50,0.5)
# Give the chart file a name.
png(file = "dbinom.png")
# Plot the graph for this sample.
plot(x,y)
# Save the file.
dev.off()

#pbinom()
# Probability of getting 26 or less heads from a 51 tosses of a coin.
x <- pbinom(26,51,0.5)
print(x)

#qbinom
# How many heads will have a probability of 0.25 will come out when a coin
# is tossed 51 times.
x <- qbinom(0.25,51,1/2)
print(x)

#rbinom
# Find 8 random values from a sample of 150 with probability of 0.4.
x <- rbinom(8,150,.4)
print(x)
```

##->Practical 7 Principal Components Analysis (PCA)<-

```
data("iris")
head(iris)
summary(iris)
myPrc<- prcomp(iris[,-5],scale=TRUE)
plot(iris$Sepal.Length,iris$Sepal.Width)
plot(scale(iris$Sepal.Length), scale(iris$Sepal.Width))
myPrc
summary(myPrc)
plot(myPrc,type='1')
biplot(myPrc,scale=0)
str(myPrc)
myPrc$x
iris2<-cbind(iris,myPrc$x[,1:2])
head(iris2)
library(ggplot2)
ggplot(iris2,aes(PC1,PC2,col=Species,fill=Species))+
  stat_ellipse(geom="polygon",col="red",alpha=0.5)+
  geom_point(shape=21,col="black")
cor(iris2[,-5],iris2[,6:7])
```

##->Practical 9 Time Series Forecasting<-

```
data("AirPassengers")
class(AirPassengers)
start(AirPassengers)
end(AirPassengers)
frequency(AirPassengers)
summary(AirPassengers)
plot(AirPassengers)
abline(reg=lm(AirPassengers~time(AirPassengers)))
cycle(AirPassengers)
plot(aggregate(AirPassengers,FUN=mean))
boxplot(AirPassengers~cycle(AirPassengers))
```

##->Practical 10 K-Means Clustering<-

```
install.packages('forecast')
library(forecast)
data("AirPassengers")
class(AirPassengers)
#Display the Dataset 
AirPassengers
#Let’s check on our date values
start(AirPassengers)
end(AirPassengers)
#So, our start date is January 1949, while the end date is December 1960.
#Find out If There Are Any Missing Values
sum(is.na(AirPassengers))
#Check the Summary of the Dataset
summary(AirPassengers)
# Plot the Dataset 
plot(AirPassengers)
#Decompose the Data Into Four Components
tsdata <- ts(AirPassengers, frequency = 12)
ddata <- decompose(tsdata, "multiplicative")
plot(ddata) 
#Plot the Different Components Individually
plot(ddata$trend)
plot(ddata$seasonal)
plot(ddata$random)
#Plot a Trendline on the Original Dataset
plot(AirPassengers)
abline(reg=lm(AirPassengers~time(AirPassengers))) 
#Create a Box Plot by Cycle
boxplot(AirPassengers~cycle(AirPassengers, xlab="Date", ylab = "Passenger Numbers
(1000's)", main = "Monthly air passengers boxplot from 1949-1960"))
```

##->Practical 11 Hierarchical Clustering<-

```
# Loading package
library(dplyr)
#Summary of dataset in package head
(mtcars)
# Finding distance matrix
distance_mat <- dist(mtcars, method = 'euclidean')
distance_mat
# Fitting Hierarchical clustering Model 
# to training dataset
# Setting seed
set.seed(240) 
Hierar_cl <- hclust(distance_mat, method = "average")
Hierar_cl
# Plotting dendrogram 
plot(Hierar_cl)
# Choosing no. of clusters 
# Cutting tree by height
abline(h = 110, col = "green")
# Cutting tree by no. of clusters 
fit <- cutree(Hierar_cl, k = 3 )
fit 
table(fit)
rect.hclust(Hierar_cl, k = 3, border = "green")
```
